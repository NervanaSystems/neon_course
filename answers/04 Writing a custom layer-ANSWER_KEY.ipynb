{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a new layer\n",
    "\n",
    "This notebook will guide you through implementing a custom layer in neon, as well as a custom activation function. You will learn\n",
    "* general interface for defining new layers\n",
    "* using the nervana backend functions\n",
    "\n",
    "## Preamble\n",
    "The first step is to set up our compute backend, and initialize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import neon\n",
    "print neon.__version__\n",
    "\n",
    "# use a GPU backend\n",
    "from neon.backends import gen_backend\n",
    "be = gen_backend('gpu', batch_size=128)\n",
    "\n",
    "# load data\n",
    "from neon.data import MNIST\n",
    "\n",
    "mnist = MNIST(path='../data/')\n",
    "train_set = mnist.train_iter\n",
    "test_set = mnist.valid_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own layer\n",
    "Instead of importing the neon supplied `Affine` Layer, we will instead build our own.\n",
    "\n",
    "Note: `Affine` is actually a compound layer; it bundles a linear layer with a bias transform and an activation function.  The `Linear` layer is what implements a fully connected layer.  \n",
    "\n",
    "First, lets build our own linear layer, called `MyLinear`, and then we will wrap that layer in a compound layer `MyAffine`.\n",
    "\n",
    "There are several important components to a layer in neon:\n",
    "* `configure`: during model initialization, this layer will receive the previous layer's object and use it to set this model's `in_shape` and `out_shape` attributes.\n",
    "* `allocate`: after each layer's shape is configured, this layer's shape information will be used to allocate memory for the output activations from `fprop`.\n",
    "* `fprop`: forward propagation. Should return a tensor with shape equal to the layer's `out_shape` attribute.\n",
    "* `bprop`: backward propagation.\n",
    "\n",
    "In the implementation below, `fprop` is implemented using element-wise operations.  It will be very slow.  Try replacing it with the neon backend implementation of `compound_dot`, such as in the `bprop` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.layers.layer import ParameterLayer, interpret_in_shape\n",
    "\n",
    "# Subclass from ParameterLayer, which handles the allocation\n",
    "# of memory buffers for the output activations, weights, and \n",
    "# bprop deltas.\n",
    "class MyLinear(ParameterLayer):\n",
    "\n",
    "    def __init__(self, nout, init, name=None):\n",
    "        super(MyLinear, self).__init__(init, name, \"Disabled\")\n",
    "        self.nout = nout\n",
    "        \n",
    "        # required attributes\n",
    "        self.inputs = None  #..?\n",
    "        self.in_shape = None  # shape of the inputs to this layer\n",
    "        self.out_shape = None  # shape of the outputs from this layer\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Linear Layer '%s': %d inputs, %d outputs\" % (\n",
    "               self.name, self.nin, self.nout)\n",
    "\n",
    "    def configure(self, in_obj):\n",
    "        \"\"\"\n",
    "        Configure the layer's input shape and output shape attributes. This is\n",
    "        required for allocating the output buffers.\n",
    "        \"\"\"\n",
    "        super(MyLinear, self).configure(in_obj)\n",
    "        \n",
    "        # shape of the input is in (# input features, batch_size)\n",
    "        (self.nin, self.nsteps) = interpret_in_shape(self.in_shape)\n",
    "        \n",
    "        # shape of the output is (# output units, batch_size)\n",
    "        self.out_shape = (self.nout, self.nsteps)\n",
    "        \n",
    "        # if the shape of the weights have not been allocated,\n",
    "        # we know that his layer's W is a tensor of shape (# outputs, # inputs).\n",
    "        if self.weight_shape is None:\n",
    "            self.weight_shape = (self.nout, self.nin)\n",
    "      \n",
    "        return self\n",
    "    \n",
    "    # We use the superclass' allocate() method.\n",
    "    # for a general layer, where you may have other memory allocations\n",
    "    # needed for computations, you can implement allocate() with\n",
    "    # your own variables.\n",
    "    #\n",
    "    # def allocate(self)\n",
    "\n",
    "    # fprop function\n",
    "    # * inference flag can be used to not store activations that may be unneeded\n",
    "    # * beta...?\n",
    "    def fprop(self, inputs, inference=False, beta=0.0):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # here we compute y = W*X inefficiently using the backend functions\n",
    "        # try substituting this with the backend `compound_dot` function to see\n",
    "        # the speed-up from using a custom kernel!\n",
    "        for r in range(self.outputs.shape[0]):\n",
    "            for c in range(self.outputs.shape[1]):\n",
    "                self.outputs[r,c] = self.be.sum(self.be.multiply(self.W[r], self.inputs[:,c].T))\n",
    "    \n",
    "        # self.be.compound_dot(A=self.W, B=self.inputs, C=self.outputs, beta=beta)\n",
    "        return self.outputs\n",
    "\n",
    "    def bprop(self, error, alpha=1.0, beta=0.0):\n",
    "        \n",
    "        # to save you headache, we use the backend compound_dot function here to compute\n",
    "        # the back-propogated deltas = W^T*error.\n",
    "        if self.deltas:\n",
    "            self.be.compound_dot(A=self.W.T, B=error, C=self.deltas, alpha=alpha, beta=beta)\n",
    "        self.be.compound_dot(A=error, B=self.inputs.T, C=self.dW)\n",
    "        return self.deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the above layer in a container, which bundles an activation and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.layers.layer import CompoundLayer\n",
    "class MyAffine(CompoundLayer):\n",
    "\n",
    "    def __init__(self, nout, init, bias=None,\n",
    "                 batch_norm=False, activation=None, name=None):\n",
    "        super(MyAffine, self).__init__(bias=bias, activation=activation, name=name)\n",
    "        self.append(MyLinear(nout, init, name=name))\n",
    "        self.add_postfilter_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an activation function (transform)\n",
    "\n",
    "We can understand more the backend functions by implementing our own softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.transforms.transform import Transform\n",
    "\n",
    "class MySoftmax(Transform):\n",
    "    \"\"\"\n",
    "    SoftMax activation function. Ensures that the activation output sums to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None, epsilon=2**-23):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        Arguments:\n",
    "            name (string, optional): Name (default: none)\n",
    "            epsilon (float, optional): Not used.\n",
    "        \"\"\"\n",
    "        super(MySoftmax, self).__init__(name)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Implement the softmax function. The input has shape (# features, batch_size) and\n",
    "        the desired output is (# features, batch_size), but where the features sum to 1.\n",
    "        We use the formula:\n",
    "        \n",
    "        f(x) = e^(x-max(x)) / sum(e^(x-max(x))) \n",
    "        \"\"\"\n",
    "        return (self.be.reciprocal(self.be.sum(\n",
    "                self.be.exp(x - self.be.max(x, axis=0)), axis=0)) *\n",
    "                self.be.exp(x - self.be.max(x, axis=0)))\n",
    "\n",
    "    def bprop(self, x):\n",
    "        \"\"\"\n",
    "        We take a shortcut here- the derivative cancels out with the CrossEntropy term.\n",
    "        \"\"\"\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together all of the pieces\n",
    "The architecture here is the same as in the `mnist_mlp.py example`, instead here we use our own `MyAffine` layer and `MySoftmax` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.initializers import Gaussian\n",
    "from neon.models import Model\n",
    "from neon.transforms.activation import Rectlin\n",
    "\n",
    "init_norm = Gaussian(loc=0.0, scale=0.01)\n",
    "\n",
    "# assemble all of the pieces\n",
    "layers = []\n",
    "layers.append(MyAffine(nout=100, init=init_norm, activation=Rectlin()))\n",
    "layers.append(MyAffine(nout=10, init=init_norm, activation=MySoftmax()))\n",
    "\n",
    "# initialize model object\n",
    "mlp = Model(layers=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit\n",
    "Using Cross Entropy loss and Gradient Descent optimizer, train the model. This will be slow, because our fprop is inefficient. Replace the fprop function using the backend's `compound_dot` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.layers import GeneralizedCost\n",
    "from neon.transforms import CrossEntropyMulti\n",
    "from neon.optimizers import GradientDescentMomentum\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "optimizer = GradientDescentMomentum(0.1, momentum_coef=0.9)\n",
    "callbacks = Callbacks(mlp, eval_set=test_set)\n",
    "\n",
    "mlp.fit(train_set, optimizer=optimizer, num_epochs=10, cost=cost,\n",
    "        callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
