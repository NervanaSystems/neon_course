{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fine-tuning VGG on CIFAR-10\n",
    "\n",
    "One of the most common questions we get is how to use neon to load a pre-trained model and fine-tune on a new dataset. In this tutorial, we show how to load a pre-trained convolutional neural network (VGG), which was trained on ImageNet, a large corpus of natural images with 1000 categories. We will then use this model to train on the CIFAR-10 dataset, a much smaller set of images with 10 categories.\n",
    "\n",
    "We begin by first generating a computational backend with the `gen_backend` function from neon. If there is a GPU available (recommended), this function will generate a GPU backend. Otherwise, a CPU backend will be used.\n",
    "\n",
    "**Note: VGG will not fit on a Kepler GPU, so here we use CPU backend for instructional purposes. If you are running on a Maxwell+ GPU, switch the backend to gpu below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend(batch_size=64, backend='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the backend via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the VGG Model\n",
    "We begin by first generating our VGG network. [VGG](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) is a popular convolutional neural network with ~19 layers. Not only does this network perform well with fine-tuning, but is also easy to define since the convolutional layers all have 3x3 filter sizes, and only differ in the number of feature maps. \n",
    "\n",
    "We first define some common parameters used by all the convolution layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.transforms import Rectlin\n",
    "from neon.initializers import Constant, Xavier\n",
    "\n",
    "relu = Rectlin()\n",
    "conv_params = {'strides': 1,\n",
    "               'padding': 1,\n",
    "               'init': Xavier(local=True),\n",
    "               'bias': Constant(0),\n",
    "               'activation': relu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define our network as a list of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.layers import Conv, Dropout, Pooling, GeneralizedCost, Affine\n",
    "from neon.initializers import GlorotUniform\n",
    "\n",
    "\n",
    "# Set up the model layers\n",
    "vgg_layers = []\n",
    "\n",
    "# set up 3x3 conv stacks with different number of filters\n",
    "vgg_layers.append(Conv((3, 3, 64), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 64), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 128), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 128), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Affine(nout=4096, init=GlorotUniform(), bias=Constant(0), activation=relu))\n",
    "vgg_layers.append(Dropout(keep=0.5))\n",
    "vgg_layers.append(Affine(nout=4096, init=GlorotUniform(), bias=Constant(0), activation=relu))\n",
    "vgg_layers.append(Dropout(keep=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of VGG is an `Affine` layer with 1000 units, for the 1000 categories in the ImageNet dataset. However, since our dataset only has 10 classes, we will instead use 10 output units. We also give this layer a special name (`class_layer`) so we know not to load pre-trained weights for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.transforms import Softmax\n",
    "\n",
    "vgg_layers.append(Affine(nout=10, init=GlorotUniform(), bias=Constant(0), activation=Softmax(),\n",
    "                  name=\"class_layer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to load the pre-trained weights into this model. First we generate a `Model` object to hold the VGG layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.models import Model\n",
    "\n",
    "model = Model(layers=vgg_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained weights\n",
    "Next, we download the pre-trained VGG weights from our [Model Zoo](https://github.com/NervanaSystems/ModelZoo/tree/master/ImageClassification/ILSVRC2012/VGG). Note: this file is quite large (~550MB). By default, the weights file is saved in your home directory. To change this, or if you have already downloaded the file somewhere else, please edit the `filepath` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.data.datasets import Dataset\n",
    "from neon.util.persist import load_obj\n",
    "import os\n",
    "\n",
    "# location and size of the VGG weights file\n",
    "url = 'https://s3-us-west-1.amazonaws.com/nervana-modelzoo/VGG/'\n",
    "filename = 'VGG_D.p'\n",
    "size = 554227541\n",
    "\n",
    "# edit filepath below if you have the file elsewhere\n",
    "_, filepath = Dataset._valid_path_append('data', '', filename)\n",
    "if not os.path.exists(filepath):\n",
    "    Dataset.fetch_dataset(url, filename, filepath, size)\n",
    "\n",
    "# load the weights param file\n",
    "print(\"Loading VGG weights from {}...\".format(filepath))\n",
    "trained_vgg = load_obj(filepath)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In neon, models are saved as python dictionaries. Below are some example calls to explore the model. You can examine the weights, the layer configuration, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The dictionary has the following keys: {}\".format(trained_vgg.keys()))\n",
    "\n",
    "layer0 = trained_vgg['model']['config']['layers'][0]\n",
    "print(\"The first layer is of type: {}\".format(layer0['type']))\n",
    "\n",
    "# filter weights of the first layer\n",
    "W = layer0['params']['W']\n",
    "print(\"The first layer weights have average magnitude of {:.2}\".format(abs(W).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to use the below blank code cell to explore the model dictionary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then iterate over the layers in our model, and load the weights from `trained_vgg` using each layers' `layer.load_weights` method. The final Affine layer is different between our model and the pre-trained model, since the number of classes have changed. Therefore, we break the loop when we encounter the final Affine layer, which has the name `class_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_layers = [l for l in model.layers.layers]\n",
    "param_dict_list = trained_vgg['model']['config']['layers']\n",
    "for layer, params in zip(param_layers, param_dict_list):\n",
    "    if(layer.name == 'class_layer'):\n",
    "        break\n",
    "\n",
    "    # To be sure, we print the name of the layer in our model \n",
    "    # and the name in the vgg model.\n",
    "    print(layer.name + \", \" + params['config']['name'])\n",
    "    layer.load_weights(params, load_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, the above code should have printed out pairs of layer names, from our model and the pre-trained vgg models. The exact name may differ, but the type of layer and layer number should match between the two.\n",
    "\n",
    "## Fine-tuning VGG on the CIFAR-10 dataset\n",
    "Now that we've modified the model for our new CIFAR-10 dataset, and loaded the model weights, let's give training a try!\n",
    "\n",
    "## Aeon Dataloader\n",
    "The CIFAR-10 dataset is small enough to fit into memory, meaning that we would normally use an `ArrayIterator` to generate our dataset. However, the CIFAR-10 images are 28x28, and VGG was trained on ImageNet, which ahs images that are of 224x224 size. For this reason, we use our macrobatching dataloader [aeon](http://aeon.nervanasys.com/), which performs image scaling and cropping on-the-fly. \n",
    "\n",
    "To prepare the data, we first invoke an ingestion script that downloads the data and creates the macrobatches. The script is located in your neon folder. Here we use some python language to extract the path to neon from the virtual environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import neon\n",
    "import os\n",
    "neon_path = os.path.split(os.path.dirname(neon.__file__))[0]\n",
    "print \"Found path to neon as {}\".format(neon_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we execute the ingestion script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run $neon_path/examples/cifar10_msra/data.py --out_dir data/cifar10/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aeon configuration\n",
    "\n",
    "Aeon allows a diverse set of configurations to specify which transformations are applied on-the-fly during training. These configs are specific as python dictionaries. For more detail, see the [aeon documentation](aeon.nervanasys.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'manifest_filename': 'data/cifar10/train-index.csv',  # CSV manifest of data\n",
    "    'manifest_root': 'data/cifar10',  # root data directory\n",
    "    'image': {'height': 224, 'width': 224,  # output image size \n",
    "              'scale': [0.875, 0.875],  # random scaling of image before cropping\n",
    "              'flip_enable': True},  # randomly flip image\n",
    "    'type': 'image,label',  # type of data\n",
    "    'minibatch_size': be.bsz  # batch size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use this configuration to create our dataloader. The outputs from the dataloader are then sent through a series of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.data.aeon_shim import AeonDataLoader\n",
    "from neon.data.dataloader_transformers import OneHot, TypeCast, BGRMeanSubtract\n",
    "\n",
    "train_set = AeonDataLoader(config, be)\n",
    "\n",
    "train_set = OneHot(train_set, index=1, nclasses=10)  # perform onehot on the labels\n",
    "train_set = TypeCast(train_set, index=0, dtype=np.float32)  # cast the image to float32\n",
    "train_set = BGRMeanSubtract(train_set, index=0)  # subtract image color means (based on default values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimizer configuration\n",
    "\n",
    "For fine-tuning, we want the final `Affine` layer to be updated with a higher learning rate compared to the pre-trained weights throughout the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.optimizers import GradientDescentMomentum, Schedule, MultiOptimizer\n",
    "from neon.transforms import CrossEntropyMulti\n",
    "# define different optimizers for the class_layer and the rest of the network\n",
    "# we use a momentum coefficient of 0.9 and weight decay of 0.0005.\n",
    "opt_vgg = GradientDescentMomentum(0.001, 0.9, wdecay=0.0005)\n",
    "opt_class_layer = GradientDescentMomentum(0.01, 0.9, wdecay=0.0005)\n",
    "\n",
    "# also define optimizers for the bias layers, which have a different learning rate\n",
    "# and not weight decay.\n",
    "opt_bias = GradientDescentMomentum(0.002, 0.9)\n",
    "opt_bias_class = GradientDescentMomentum(0.02, 0.9)\n",
    "\n",
    "# set up the mapping of layers to optimizers\n",
    "opt = MultiOptimizer({'default': opt_vgg, 'Bias': opt_bias,\n",
    "     'class_layer': opt_class_layer, 'class_layer_bias': opt_bias_class})\n",
    "\n",
    "# use cross-entropy cost to train the network\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up callbacks so the model can report progress during training, and then run the `model.fit` function. Note that if you are on a CPU, this next section will take long to finish for you to see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.callbacks.callbacks import Callbacks\n",
    "\n",
    "callbacks = Callbacks(model)\n",
    "model.fit(train_set, optimizer=opt, num_epochs=10, cost=cost, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the network cost decrease over the course of training as it fine-tunes on this new dataset.\n",
    "\n",
    "## Customize for your own data\n",
    "With neon it is easy to adapt this notebook to fine-tune on your own dataset -- most of the code above can be reused regardless of your specific problem. There just a few things to do:\n",
    "1. Organize your image dataset into a format that our macrobatching loader recognizes. Run our `batch_writer.py` script to generate the macrobatches. See [Macrobatching](http://neon.nervanasys.com/docs/latest/loading_data.html#macrobatching) in our documentation for more information.\n",
    "2. Modify the number of output units in the last `Affine` layer of the network to match the number of categories in your dataset.\n",
    "\n",
    "Feel free to visit our [github](https://github.com/NervanaSystems/neon/) page or our [documention](http://neon.nervanasys.com/docs/latest/index.html) if you have questions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
