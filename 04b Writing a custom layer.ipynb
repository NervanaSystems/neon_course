{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a new layer\n",
    "\n",
    "This notebook will guide you through implementing a custom layer in neon, as well as a custom activation function. You will learn\n",
    "* general interface for defining new layers\n",
    "* using the nervana backend functions\n",
    "\n",
    "## Preamble\n",
    "The first step is to set up our compute backend, and initialize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import neon\n",
    "\n",
    "# use a GPU backend\n",
    "from neon.backends import gen_backend\n",
    "be = gen_backend('gpu', batch_size=128)\n",
    "\n",
    "# load data\n",
    "from neon.data import load_mnist\n",
    "from neon.data import ArrayIterator\n",
    "\n",
    "# download or reuse cached data\n",
    "(X_train, y_train), (X_test, y_test), nclass = load_mnist(path='data')\n",
    "\n",
    "# setup training and test set iterator\n",
    "train_set = ArrayIterator(X_train, y_train, nclass=nclass)\n",
    "test_set = ArrayIterator(X_test, y_test, nclass=nclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own layer\n",
    "Instead of importing the neon supplied `Affine` Layer, we will instead build our own.\n",
    "\n",
    "Note: `Affine` is actually a compound layer; it bundles a linear layer with a bias transform and an activation function.  The `Linear` layer is what implements a fully connected layer.  \n",
    "\n",
    "First, lets build our own linear layer, called `MyLinear`, and then we will wrap that layer in a compound layer `MyAffine`.\n",
    "\n",
    "There are several important components to a layer in neon:\n",
    "* `configure`: during model initialization, this layer will receive the previous layer's object and use it to set this model's `in_shape` and `out_shape` attributes.\n",
    "* `allocate`: after each layer's shape is configured, this layer's shape information will be used to allocate memory for the output activations from `fprop`.\n",
    "* `fprop`: forward propagation. Should return a tensor with shape equal to the layer's `out_shape` attribute.\n",
    "* `bprop`: backward propagation.\n",
    "\n",
    "In the implementation below, `fprop` is implemented using element-wise operations.  It will be very slow.  Try replacing it with the neon backend implementation of `compound_dot`, such as in the `bprop` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.layers.layer import ParameterLayer, interpret_in_shape\n",
    "\n",
    "# Subclass from ParameterLayer, which handles the allocation\n",
    "# of memory buffers for the output activations, weights, and \n",
    "# bprop deltas.\n",
    "class MyLinear(ParameterLayer):\n",
    "\n",
    "    def __init__(self, nout, init, name=None):\n",
    "        super(MyLinear, self).__init__(init, name, \"Disabled\")\n",
    "        self.nout = nout\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Linear Layer '%s': %d inputs, %d outputs\" % (\n",
    "               self.name, self.nin, self.nout)\n",
    "\n",
    "    def configure(self, in_obj):\n",
    "        super(MyLinear, self).configure(in_obj)\n",
    "        \n",
    "        # shape of the input is in (# input features, batch_size)\n",
    "        (self.nin, self.nsteps) = interpret_in_shape(self.in_shape)\n",
    "        \n",
    "        # shape of the output is (# output units, batch_size)\n",
    "        self.out_shape = (self.nout, self.nsteps)\n",
    "        \n",
    "        # if the shape of the weights have not been allocated,\n",
    "        # we know that his layer's W is a tensor of shape (# outputs, # inputs).\n",
    "        if self.weight_shape is None:\n",
    "            self.weight_shape = (self.nout, self.nin)\n",
    "      \n",
    "        return self\n",
    "\n",
    "    def fprop(self, inputs, inference=False, beta=0.0):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # here we compute y = W*X inefficiently using the backend functions\n",
    "        for r in range(self.outputs.shape[0]):\n",
    "            for c in range(self.outputs.shape[1]):\n",
    "                self.outputs[r,c] = self.be.sum(self.W[r,:] * inputs[:,c].T)\n",
    "                \n",
    "        # TODO:\n",
    "        # try substituting the for loops above with the backend `compound_dot` \n",
    "        # function to see the speed-up from using a custom gpu kernel!\n",
    "        # self.be.compound_dot(A=self.W, B=inputs, C=self.outputs)\n",
    "        \n",
    "        return self.outputs\n",
    "\n",
    "    def bprop(self, error, alpha=1.0, beta=0.0):\n",
    "        \n",
    "        # to save you headache, we use the backend compound_dot function here to compute\n",
    "        # the back-propogated deltas = W^T*error.\n",
    "        if self.deltas:\n",
    "            self.be.compound_dot(A=self.W.T, B=error, C=self.deltas, alpha=alpha, beta=beta)\n",
    "        self.be.compound_dot(A=error, B=self.inputs.T, C=self.dW)\n",
    "        return self.deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the above layer in a container, which bundles an activation and batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together all of the pieces\n",
    "The architecture here is the same as in the `mnist_mlp.py example`, instead here we use our own `MyAffine` layer and `MySoftmax` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.initializers import Gaussian\n",
    "from neon.models import Model\n",
    "from neon.layers.layer import Activation\n",
    "from neon.transforms.activation import Rectlin, Softmax\n",
    "\n",
    "init_norm = Gaussian(loc=0.0, scale=0.01)\n",
    "\n",
    "# assemble all of the pieces\n",
    "layers = []\n",
    "layers.append(MyLinear(nout=100, init=init_norm, name=\"Linear100\"))\n",
    "layers.append(Activation(Rectlin()))\n",
    "\n",
    "layers.append(MyLinear(nout=10, init=init_norm, name=\"Linear10\"))\n",
    "layers.append(Activation(Softmax()))\n",
    "\n",
    "# initialize model object\n",
    "mlp = Model(layers=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit\n",
    "Using Cross Entropy loss and Gradient Descent optimizer, train the model. This will be slow, because our fprop is inefficient. Replace the fprop function using the backend's `compound_dot` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.layers import GeneralizedCost\n",
    "from neon.transforms import CrossEntropyMulti\n",
    "from neon.optimizers import GradientDescentMomentum\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "optimizer = GradientDescentMomentum(0.1, momentum_coef=0.9)\n",
    "callbacks = Callbacks(mlp, eval_set=test_set)\n",
    "\n",
    "mlp.fit(train_set, optimizer=optimizer, num_epochs=10, cost=cost,\n",
    "        callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
